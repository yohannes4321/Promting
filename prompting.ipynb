{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b6SrQEHw04nX"
      },
      "outputs": [],
      "source": [
        "pip install langchain_google_genai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "RzrC50-41gRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMNI_API_KEY=userdata.get('GEMNI_API_KEY')"
      ],
      "metadata": {
        "id": "JTb2yHdt3EkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    api_key=GEMNI_API_KEY,\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"Enter your prompt here\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "xWDGtyyg1Y3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundamental Prompting Techniques1 Role Prompting"
      ],
      "metadata": {
        "id": "un8ArIBs7y1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"You are a shoes critic  . Write a review of Nike shoes.\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "ZLgflqKW3LxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-Shot Prompting use cases"
      ],
      "metadata": {
        "id": "Z_j7AxLu8w18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Provide a possible diagnosis and explain your reasoning:Symptoms: Fever, cough, fatigueDiagnosis: Common coldExplanation: The combination of fever, cough, and fatigue is typical of a common cold. No severe symptoms are present, suggesting a mild viral infection.Symptoms: Chest pain, shortness of breath, dizzinessDiagnosis: Possible heart attackExplanation: The combination of chest pain, shortness of breath, and dizziness are warning signs of a possible heart attack. Immediate medical attention is required.Symptoms: Headache, sensitivity to light, nauseaDiagnosis:Explanation: \"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "lP3HgL4t35T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"def calculate_area(length: float, width: float) -> float:\n",
        "    \\\"\\\"\\\"\n",
        "    Calculate the area of a rectangle.\n",
        "    Args:\n",
        "        length (float): The length of the rectangle.\n",
        "        width (float): The width of the rectangle.\n",
        "    Returns:\n",
        "        float: The area of the rectangle.\n",
        "    \\\"\\\"\\\"\n",
        "    return length * width\n",
        "\n",
        "def celsius_to_fahrenheit(celsius: float) -> float:\n",
        "    \\\"\\\"\\\"\n",
        "    Convert temperature from Celsius to Fahrenheit.\n",
        "    Args:\n",
        "        celsius (float): Temperature in Celsius.\n",
        "    Returns:\n",
        "        float: Temperature in Fahrenheit.\n",
        "    \\\"\\\"\\\"\n",
        "    return (celsius * 9/5) + 32\n",
        "based on this calclulate for bmi\n",
        "def calculate_bmi(weight: float, height: float) -> float:\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    \\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "lov-9LC-37lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain-of-Thought Prompting\n",
        "step by step reasoning"
      ],
      "metadata": {
        "id": "WT2SdpW-EIYW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TISgxZEKFAZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chain of thougt prompting"
      ],
      "metadata": {
        "id": "kLaowhIeFA-8"
      }
    },
    {
      "source": [
        "prompt=\"\"\"Question: Roger has 5 tennis balls he buys 2 more cans of tennis balls each can\n",
        " has 3 tennis balls how many tennis balls does he have now?\n",
        "answer:  Roger started with 5 balles 2 cans of 3 tennnis balls each is 6 tennis balls 5+6 = 11 tennis balls. the answer is 11\n",
        "\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)\n",
        "Question: The cafeteria had 23 apples if they used 20 to make lunch and bought\n",
        "6 more how many apples do they have ?\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_H85xxtv4f_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"Which is a faster way to get home?\n",
        "Option 1: Take an 10 minutes bus, then an 40 minute bus, and finally a 10 minute train.\n",
        "Option 2: Take a 90 minutes train, then a 45 minute bike ride, and finally a 10 minute bus.\n",
        "Option 1 will take 10+40+10 = 60 minutes.\n",
        "Option 2 will take 90+45+10=145 minutes.\n",
        "Since Option 1 takes 60 minutes and Option 2 takes 145 minutes, Option 1 is faster.\n",
        "\n",
        "Which is a faster way to get to work?\n",
        "Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.\n",
        "Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "939sxmFv4NXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"When I was 6 my sister was half my age. Now\n",
        "Iâ€™m 70 how old is my sister?\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "qjB8zmNLFYlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"classify the recipe of helthiness as HEALTHY ,MODERATE,UNHEALTHY based on nutiritional impact\n",
        "\n",
        "1/2 cup butter 1 cup shuger 2 cup flour 1 cup chocolate chips 1/2 cup vanilla 4 egg 1/2 teaspoon beaking poweder\"\"\"\n",
        "print(llm.invoke(prompt).content)\n",
        "print(llm.invoke(prompt).content)\n",
        "print(llm.invoke(prompt).content)\n",
        "\n"
      ],
      "metadata": {
        "id": "1zjMNZEtLKCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dual prompt\n"
      ],
      "metadata": {
        "id": "zdx7FvIUPmG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"I am writing a guide about SEO. Give me 10 key topics that I should cover in this guide.\"\n",
        "result1 = llm.invoke(prompt1).content\n",
        "\n",
        "# Now format result1 properly and insert it into the second prompt\n",
        "prompt2 = f\"Write me a detailed guide about each of the points you gave in {result1}\"\n",
        "result2 = llm.invoke(prompt2).content\n",
        "\n",
        "print(result2)\n"
      ],
      "metadata": {
        "id": "paihYiSzN9Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-experimental\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KPZyppdLWaNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program Aided Language(PAL)"
      ],
      "metadata": {
        "id": "ENA89VdQVbc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.pal_chain import PALChain\n"
      ],
      "metadata": {
        "id": "vVDV3uUoSSoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palchain =PALChain.from_math_prompt(llm,verbose=True,allow_dangerous_code=True)"
      ],
      "metadata": {
        "id": "1IaqI8PbPwXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"\"\"Jan has three times the number of pets as Marcia.\n",
        "Marcia has two more pets than Cindy.\n",
        "If Cindy has four pets, how many pets do the three have?\"\"\"\n",
        "llm_out=palchain.invoke(question)\n",
        "exec(llm_out)\n",
        "print(born)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h8HpAtlsSd2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReACT"
      ],
      "metadata": {
        "id": "8DmOoFLw34Pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DGSk9Oif34pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade python-dotenv\n",
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "AXFmB8XNaZdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from google.colab import userdata\n",
        "serper_api_key=userdata.get('serper')\n",
        "os.environ[\"SERPER_API_KEY\"] = serper_api_key\n"
      ],
      "metadata": {
        "id": "3dSP29LKSg_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "Sr2FAGZZSsCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
      ],
      "metadata": {
        "id": "W7AlWwdldYlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cD5hoA2cnkfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree Prompting\n"
      ],
      "metadata": {
        "id": "snFUJk_k1egN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts=\"\"\"Imagine three different experts are answering this question.\n",
        "All experts will write down 1 step of their thinking,\n",
        "then share it with the group.\n",
        "Then all experts will go on to the next step, etc.\n",
        "If any expert realises they're wrong at any point then they leave.\n",
        "The question is...\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "WJKa3LRg1glm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directional Stimulus Prompting\n",
        "A tuneable policy LM is trained to generate the stimulus/hint. Seeing more use of RL to optimize LLMs.\n",
        "\n",
        "The figure below shows how Directional Stimulus Prompting compares with standard prompting. The policy LM can be small and optimized to generate the hints that guide a black-box frozen LLM."
      ],
      "metadata": {
        "id": "-1kdh-0c3Smz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article=\"\"\"Article: (CNN) For the first time in eight years, a TV legend returned to doing what he does best. Contestants told to \"come on down!\"\n",
        "on the April 1 edition of \"The Price Is Right\" encountered not host Drew Carey but another familiar face in charge of the proceedings. Instead, there was Bob Barker,\n",
        "who hosted the TV game show for 35 years before stepping down in 2007. Looking spry at 91, Barker handled the first price-guessing game of the show, the classic \"Lucky Seven,\" before turning hosting duties over to Carey,\n",
        "who finished up. Despite being away from the show for most of the past eight years, Barker didn't seem to miss a be\"\"\"\n",
        "\n",
        "\n",
        "prompt1=f\"\"\"Q: Summarize the above {article}briefly in 2-3 sentences based on the hint.\n",
        "Hint: Bob Barker; TV; April 1; \"The Price Is Right\", 2007; 91.\"\"\"\n"
      ],
      "metadata": {
        "id": "scq3Esh72KRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(prompt1)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "qFC1sB7e3hSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AuqK7YP_4HrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " multimodal chain-of-thought prompting approach. Traditional CoT focuses on the language modality. In contrast, Multimodal CoT incorporates text and vision into a two-stage framework. The first step involves rationale generation based on multimodal information. This is followed by the second phase, answer inference, which leverages the informative generated rationales."
      ],
      "metadata": {
        "id": "KxWtcc5O4dDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_r=\"\"\"Question: Which property do these two objects have in common?\n",
        "Context: Select the better answer.\n",
        "Options:\n",
        "(A) soft\n",
        "(B) salty from sncaks and biscutt in image\"\"\""
      ],
      "metadata": {
        "id": "gJuPAZ9i4dmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(prompt_r)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "SvVIWjo_40Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt chaning Prompt chaining is useful to accomplish complex tasks which an LLM might struggle to address if prompted with a very detailed prompt. In prompt chaining, chain prompts perform transformations or additional processes on the generated responses before reaching a final desired state.\n",
        "\n"
      ],
      "metadata": {
        "id": "XUjmHPin51F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document=\"\"\"Title: The Benefits of Learning to Code\n",
        "\n",
        "Learning to code has many advantages. It helps develop problem-solving skills, improves logical thinking, and opens up career opportunities in technology. Many industries now rely on coding for automation, data analysis, and software development.\n",
        "\n",
        "Popular programming languages include Python, JavaScript, and Java. Python is known for its simplicity, making it a great choice for beginners. JavaScript is essential for web development, while Java is widely used in enterprise applications.\n",
        "\n",
        "Overall, coding is a valuable skill that empowers individuals to create technology, solve real-world problems, and innovate in various fields.\"\"\""
      ],
      "metadata": {
        "id": "VvnBTg7i5e3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2=\"\"\"You are a helpful assistant. Your task is to help answer a question given in a document. The first step is to extract quotes relevant to the question from the document, delimited by ####. Please output the list of quotes using <quotes></quotes>. Respond with \"No relevant quotes found!\" if no relevant quotes were found.\n",
        "####\n",
        "{{document}}\n",
        "####\"\"\""
      ],
      "metadata": {
        "id": "SMb7SysI5huo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(prompt2)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "YGVn6cLi5p5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}