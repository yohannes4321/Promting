{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b6SrQEHw04nX"
      },
      "outputs": [],
      "source": [
        "pip install langchain_google_genai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "RzrC50-41gRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMNI_API_KEY=userdata.get('GEMNI_API_KEY')"
      ],
      "metadata": {
        "id": "JTb2yHdt3EkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    api_key=GEMNI_API_KEY,\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"Enter your prompt here\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "xWDGtyyg1Y3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundamental Prompting Techniques1 Role Prompting"
      ],
      "metadata": {
        "id": "un8ArIBs7y1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"You are a shoes critic  . Write a review of Nike shoes.\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "ZLgflqKW3LxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-Shot Prompting use cases"
      ],
      "metadata": {
        "id": "Z_j7AxLu8w18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Provide a possible diagnosis and explain your reasoning:Symptoms: Fever, cough, fatigueDiagnosis: Common coldExplanation: The combination of fever, cough, and fatigue is typical of a common cold. No severe symptoms are present, suggesting a mild viral infection.Symptoms: Chest pain, shortness of breath, dizzinessDiagnosis: Possible heart attackExplanation: The combination of chest pain, shortness of breath, and dizziness are warning signs of a possible heart attack. Immediate medical attention is required.Symptoms: Headache, sensitivity to light, nauseaDiagnosis:Explanation: \"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "lP3HgL4t35T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"def calculate_area(length: float, width: float) -> float:\n",
        "    \\\"\\\"\\\"\n",
        "    Calculate the area of a rectangle.\n",
        "    Args:\n",
        "        length (float): The length of the rectangle.\n",
        "        width (float): The width of the rectangle.\n",
        "    Returns:\n",
        "        float: The area of the rectangle.\n",
        "    \\\"\\\"\\\"\n",
        "    return length * width\n",
        "\n",
        "def celsius_to_fahrenheit(celsius: float) -> float:\n",
        "    \\\"\\\"\\\"\n",
        "    Convert temperature from Celsius to Fahrenheit.\n",
        "    Args:\n",
        "        celsius (float): Temperature in Celsius.\n",
        "    Returns:\n",
        "        float: Temperature in Fahrenheit.\n",
        "    \\\"\\\"\\\"\n",
        "    return (celsius * 9/5) + 32\n",
        "based on this calclulate for bmi\n",
        "def calculate_bmi(weight: float, height: float) -> float:\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    \\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "lov-9LC-37lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain-of-Thought Prompting\n",
        "step by step reasoning"
      ],
      "metadata": {
        "id": "WT2SdpW-EIYW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TISgxZEKFAZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chain of thougt prompting"
      ],
      "metadata": {
        "id": "kLaowhIeFA-8"
      }
    },
    {
      "source": [
        "prompt=\"\"\"Question: Roger has 5 tennis balls he buys 2 more cans of tennis balls each can\n",
        " has 3 tennis balls how many tennis balls does he have now?\n",
        "answer:  Roger started with 5 balles 2 cans of 3 tennnis balls each is 6 tennis balls 5+6 = 11 tennis balls. the answer is 11\n",
        "\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)\n",
        "Question: The cafeteria had 23 apples if they used 20 to make lunch and bought\n",
        "6 more how many apples do they have ?\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_H85xxtv4f_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"Which is a faster way to get home?\n",
        "Option 1: Take an 10 minutes bus, then an 40 minute bus, and finally a 10 minute train.\n",
        "Option 2: Take a 90 minutes train, then a 45 minute bike ride, and finally a 10 minute bus.\n",
        "Option 1 will take 10+40+10 = 60 minutes.\n",
        "Option 2 will take 90+45+10=145 minutes.\n",
        "Since Option 1 takes 60 minutes and Option 2 takes 145 minutes, Option 1 is faster.\n",
        "\n",
        "Which is a faster way to get to work?\n",
        "Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.\n",
        "Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "939sxmFv4NXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"When I was 6 my sister was half my age. Now\n",
        "Iâ€™m 70 how old is my sister?\"\"\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "qjB8zmNLFYlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"classify the recipe of helthiness as HEALTHY ,MODERATE,UNHEALTHY based on nutiritional impact\n",
        "\n",
        "1/2 cup butter 1 cup shuger 2 cup flour 1 cup chocolate chips 1/2 cup vanilla 4 egg 1/2 teaspoon beaking poweder\"\"\"\n",
        "print(llm.invoke(prompt).content)\n",
        "print(llm.invoke(prompt).content)\n",
        "print(llm.invoke(prompt).content)\n",
        "\n"
      ],
      "metadata": {
        "id": "1zjMNZEtLKCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dual prompt\n"
      ],
      "metadata": {
        "id": "zdx7FvIUPmG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"I am writing a guide about SEO. Give me 10 key topics that I should cover in this guide.\"\n",
        "result1 = llm.invoke(prompt1).content\n",
        "\n",
        "# Now format result1 properly and insert it into the second prompt\n",
        "prompt2 = f\"Write me a detailed guide about each of the points you gave in {result1}\"\n",
        "result2 = llm.invoke(prompt2).content\n",
        "\n",
        "print(result2)\n"
      ],
      "metadata": {
        "id": "paihYiSzN9Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-experimental\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KPZyppdLWaNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program Aided Language(PAL)"
      ],
      "metadata": {
        "id": "ENA89VdQVbc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.pal_chain import PALChain\n"
      ],
      "metadata": {
        "id": "vVDV3uUoSSoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palchain =PALChain.from_math_prompt(llm,verbose=True,allow_dangerous_code=True)"
      ],
      "metadata": {
        "id": "1IaqI8PbPwXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"\"\"Jan has three times the number of pets as Marcia.\n",
        "Marcia has two more pets than Cindy.\n",
        "If Cindy has four pets, how many pets do the three have?\"\"\"\n",
        "llm_out=palchain.invoke(question)\n",
        "exec(llm_out)\n",
        "print(born)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h8HpAtlsSd2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade python-dotenv\n",
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "AXFmB8XNaZdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from google.colab import userdata\n",
        "serper_api_key=userdata.get('serper')\n",
        "os.environ[\"SERPER_API_KEY\"] = serper_api_key\n"
      ],
      "metadata": {
        "id": "3dSP29LKSg_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "Sr2FAGZZSsCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
      ],
      "metadata": {
        "id": "W7AlWwdldYlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cD5hoA2cnkfk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}